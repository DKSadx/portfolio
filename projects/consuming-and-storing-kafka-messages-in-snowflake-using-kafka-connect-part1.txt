1:HL["/_next/static/media/162bf645eb375add-s.p.ttf","font",{"crossOrigin":"","type":"font/ttf"}]
2:HL["/_next/static/media/c9a5bc6a7c948fb0-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
3:HL["/_next/static/css/d2ee96617fdd9da5.css","style",{"crossOrigin":""}]
0:["Jb7UV8SQXuc1APbyc4iSV",[[["",{"children":["projects",{"children":[["slug","consuming-and-storing-kafka-messages-in-snowflake-using-kafka-connect-part1","d"],{"children":["__PAGE__?{\"slug\":\"consuming-and-storing-kafka-messages-in-snowflake-using-kafka-connect-part1\"}",{}]}]}]},"$undefined","$undefined",true],"$L4",[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/d2ee96617fdd9da5.css","precedence":"next","crossOrigin":""}]],"$L5"]]]]
6:HL["/_next/static/css/4332eee14cae2b92.css","style",{"crossOrigin":""}]
7:I{"id":4089,"chunks":["185:static/chunks/app/layout-c2a58729861ada32.js"],"name":"Analytics","async":false}
8:I{"id":1602,"chunks":["272:static/chunks/webpack-b7788230df343aa2.js","274:static/chunks/f42e5159-56eb28c3a582d27e.js","83:static/chunks/83-0ec6f3c254c9dbc7.js"],"name":"","async":false}
9:I{"id":233,"chunks":["272:static/chunks/webpack-b7788230df343aa2.js","274:static/chunks/f42e5159-56eb28c3a582d27e.js","83:static/chunks/83-0ec6f3c254c9dbc7.js"],"name":"","async":false}
5:[["$","meta","0",{"charSet":"utf-8"}],["$","title","1",{"children":"Darko Kojović | DevOps Engineer"}],["$","meta","2",{"name":"description","content":"Lead DevOps Engineer at Atlantbh"}],["$","meta","3",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","4",{"name":"robots","content":"index, follow"}],["$","meta","5",{"name":"googlebot","content":"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"}],["$","meta","6",{"property":"og:title","content":"darko.sh"}],["$","meta","7",{"property":"og:description","content":"Lead DevOps Engineer at Atlantbh"}],["$","meta","8",{"property":"og:url","content":"https://darko.sh"}],["$","meta","9",{"property":"og:site_name","content":"Darko Kojović"}],["$","meta","10",{"property":"og:locale","content":"en-US"}],["$","meta","11",{"property":"og:type","content":"website"}],["$","meta","12",{"name":"twitter:card","content":"summary"}],["$","meta","13",{"name":"twitter:title","content":"darko.sh"}],["$","meta","14",{"name":"twitter:description","content":"Lead DevOps Engineer at Atlantbh"}],["$","link","15",{"rel":"shortcut icon","href":"/favicon.png"}],["$","meta","16",{"name":"next-size-adjust"}]]
4:[null,["$","html",null,{"lang":"en","className":"__variable_aaf875 __variable_829c8b","children":[["$","head",null,{"children":["$","$L7",null,{}]}],["$","body",null,{"className":"bg-black undefined","children":["$","$L8",null,{"parallelRouterKey":"children","segmentPath":["children"],"loading":"$undefined","loadingStyles":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","template":["$","$L9",null,{}],"templateStyles":"$undefined","notFound":[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":"404"}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],"notFoundStyles":[],"childProp":{"current":[null,["$","div",null,{"className":"relative min-h-screen bg-gradient-to-tl from-zinc-900 via-zinc-400/10 to-zinc-900 ","children":["$","$L8",null,{"parallelRouterKey":"children","segmentPath":["children","projects","children"],"loading":"$undefined","loadingStyles":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","template":["$","$L9",null,{}],"templateStyles":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","childProp":{"current":["$","$L8",null,{"parallelRouterKey":"children","segmentPath":["children","projects","children",["slug","consuming-and-storing-kafka-messages-in-snowflake-using-kafka-connect-part1","d"],"children"],"loading":"$undefined","loadingStyles":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","template":["$","$L9",null,{}],"templateStyles":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","childProp":{"current":["$La","$Lb",null],"segment":"__PAGE__?{\"slug\":\"consuming-and-storing-kafka-messages-in-snowflake-using-kafka-connect-part1\"}"},"styles":[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/4332eee14cae2b92.css","precedence":"next","crossOrigin":""}]]}],"segment":["slug","consuming-and-storing-kafka-messages-in-snowflake-using-kafka-connect-part1","d"]},"styles":[]}]}],null],"segment":"projects"},"styles":[]}]}]]}],null]
c:I{"id":5415,"chunks":["499:static/chunks/499-57dd4957aac9efe1.js","903:static/chunks/903-24266959a8966958.js","101:static/chunks/app/projects/[slug]/page-4adac04e230db6ff.js"],"name":"Header","async":false}
f:I{"id":918,"chunks":["499:static/chunks/499-57dd4957aac9efe1.js","903:static/chunks/903-24266959a8966958.js","101:static/chunks/app/projects/[slug]/page-4adac04e230db6ff.js"],"name":"","async":false}
d:T68a,
Every year, zettabytes of data are transferred over the Internet. Managing, processing, and storing the data can be a really complex task and requires cutting-edge tools. Many tools can be used, but two technologies that are almost essential to every modern data pipeline: Kafka and Snowflake.

If you are planning to stream a large amount of data, Kafka is usually the best choice. Scalability, reliability, low latency and popularity are reasons why Kafka stands out. Kafka is by far the most popular event streaming platform and a go-to for many cases when it comes to streaming data.

Since you need to process and store the data somewhere, Snowflake is one of the top contenders for that. If you are wondering why Snowflake is so popular, look at the user satisfaction — it’s over the roof. Snowflake took most of the drawbacks and oversights that other data storage solutions had and addressed it. Probably the most important feature is having the storage and compute separately scaled.

### What is Kafka Connect, and why did we decide to use it?

Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other data systems. It provides streaming integration between Kafka and other data sources or sinks, enabling seamless data movement and transformation. Kafka Connect uses connectors to ingest data into Kafka topics or export data from Kafka topics to external systems.

Kafka Connect is made for use cases like this.

To see how we used Kafka Connect for consuming and storing Kafka messages in Snowflake, check out the full blog (for free) at: https://www.atlantbh.com/kafka-connect/ or https://medium.com/p/f4a14a692b3a.
e:T48bc,var Component=(()=>{var ur=Object.create;var F=Object.defineProperty;var lr=Object.getOwnPropertyDescriptor;var fr=Object.getOwnPropertyNames;var cr=Object.getPrototypeOf,dr=Object.prototype.hasOwnProperty;var q=(l,o)=>()=>(o||l((o={exports:{}}).exports,o),o.exports),pr=(l,o)=>{for(var h in o)F(l,h,{get:o[h],enumerable:!0})},ke=(l,o,h,y)=>{if(o&&typeof o=="object"||typeof o=="function")for(let g of fr(o))!dr.call(l,g)&&g!==h&&F(l,g,{get:()=>o[g],enumerable:!(y=lr(o,g))||y.enumerable});return l};var mr=(l,o,h)=>(h=l!=null?ur(cr(l)):{},ke(o||!l||!l.__esModule?F(h,"default",{value:l,enumerable:!0}):h,l)),vr=l=>ke(F({},"__esModule",{value:!0}),l);var we=q((kr,Ee)=>{Ee.exports=React});var Re=q(z=>{"use strict";(function(){"use strict";var l=we(),o=Symbol.for("react.element"),h=Symbol.for("react.portal"),y=Symbol.for("react.fragment"),g=Symbol.for("react.strict_mode"),G=Symbol.for("react.profiler"),X=Symbol.for("react.provider"),H=Symbol.for("react.context"),T=Symbol.for("react.forward_ref"),A=Symbol.for("react.suspense"),U=Symbol.for("react.suspense_list"),C=Symbol.for("react.memo"),I=Symbol.for("react.lazy"),Ce=Symbol.for("react.offscreen"),J=Symbol.iterator,Se="@@iterator";function Oe(e){if(e===null||typeof e!="object")return null;var r=J&&e[J]||e[Se];return typeof r=="function"?r:null}var E=l.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED;function p(e){{for(var r=arguments.length,a=new Array(r>1?r-1:0),t=1;t<r;t++)a[t-1]=arguments[t];Pe("error",e,a)}}function Pe(e,r,a){{var t=E.ReactDebugCurrentFrame,s=t.getStackAddendum();s!==""&&(r+="%s",a=a.concat([s]));var u=a.map(function(i){return String(i)});u.unshift("Warning: "+r),Function.prototype.apply.call(console[e],console,u)}}var Ne=!1,De=!1,Fe=!1,Ae=!1,Ue=!1,Z;Z=Symbol.for("react.module.reference");function Ie(e){return!!(typeof e=="string"||typeof e=="function"||e===y||e===G||Ue||e===g||e===A||e===U||Ae||e===Ce||Ne||De||Fe||typeof e=="object"&&e!==null&&(e.$$typeof===I||e.$$typeof===C||e.$$typeof===X||e.$$typeof===H||e.$$typeof===T||e.$$typeof===Z||e.getModuleId!==void 0))}function Ke(e,r,a){var t=e.displayName;if(t)return t;var s=r.displayName||r.name||"";return s!==""?a+"("+s+")":a}function Q(e){return e.displayName||"Context"}function b(e){if(e==null)return null;if(typeof e.tag=="number"&&p("Received an unexpected object in getComponentNameFromType(). This is likely a bug in React. Please file an issue."),typeof e=="function")return e.displayName||e.name||null;if(typeof e=="string")return e;switch(e){case y:return"Fragment";case h:return"Portal";case G:return"Profiler";case g:return"StrictMode";case A:return"Suspense";case U:return"SuspenseList"}if(typeof e=="object")switch(e.$$typeof){case H:var r=e;return Q(r)+".Consumer";case X:var a=e;return Q(a._context)+".Provider";case T:return Ke(e,e.render,"ForwardRef");case C:var t=e.displayName||null;return t!==null?t:b(e.type)||"Memo";case I:{var s=e,u=s._payload,i=s._init;try{return b(i(u))}catch{return null}}}return null}var k=Object.assign,j=0,ee,re,ae,te,ne,oe,ie;function se(){}se.__reactDisabledLog=!0;function Me(){{if(j===0){ee=console.log,re=console.info,ae=console.warn,te=console.error,ne=console.group,oe=console.groupCollapsed,ie=console.groupEnd;var e={configurable:!0,enumerable:!0,value:se,writable:!0};Object.defineProperties(console,{info:e,log:e,warn:e,error:e,group:e,groupCollapsed:e,groupEnd:e})}j++}}function We(){{if(j--,j===0){var e={configurable:!0,enumerable:!0,writable:!0};Object.defineProperties(console,{log:k({},e,{value:ee}),info:k({},e,{value:re}),warn:k({},e,{value:ae}),error:k({},e,{value:te}),group:k({},e,{value:ne}),groupCollapsed:k({},e,{value:oe}),groupEnd:k({},e,{value:ie})})}j<0&&p("disabledDepth fell below zero. This is a bug in React. Please file an issue.")}}var K=E.ReactCurrentDispatcher,M;function S(e,r,a){{if(M===void 0)try{throw Error()}catch(s){var t=s.stack.trim().match(/\n( *(at )?)/);M=t&&t[1]||""}return`
`+M+e}}var W=!1,O;{var Ye=typeof WeakMap=="function"?WeakMap:Map;O=new Ye}function ue(e,r){if(!e||W)return"";{var a=O.get(e);if(a!==void 0)return a}var t;W=!0;var s=Error.prepareStackTrace;Error.prepareStackTrace=void 0;var u;u=K.current,K.current=null,Me();try{if(r){var i=function(){throw Error()};if(Object.defineProperty(i.prototype,"props",{set:function(){throw Error()}}),typeof Reflect=="object"&&Reflect.construct){try{Reflect.construct(i,[])}catch(_){t=_}Reflect.construct(e,[],i)}else{try{i.call()}catch(_){t=_}e.call(i.prototype)}}else{try{throw Error()}catch(_){t=_}e()}}catch(_){if(_&&t&&typeof _.stack=="string"){for(var n=_.stack.split(`
`),m=t.stack.split(`
`),f=n.length-1,c=m.length-1;f>=1&&c>=0&&n[f]!==m[c];)c--;for(;f>=1&&c>=0;f--,c--)if(n[f]!==m[c]){if(f!==1||c!==1)do if(f--,c--,c<0||n[f]!==m[c]){var v=`
`+n[f].replace(" at new "," at ");return e.displayName&&v.includes("<anonymous>")&&(v=v.replace("<anonymous>",e.displayName)),typeof e=="function"&&O.set(e,v),v}while(f>=1&&c>=0);break}}}finally{W=!1,K.current=u,We(),Error.prepareStackTrace=s}var R=e?e.displayName||e.name:"",ye=R?S(R):"";return typeof e=="function"&&O.set(e,ye),ye}function $e(e,r,a){return ue(e,!1)}function Ve(e){var r=e.prototype;return!!(r&&r.isReactComponent)}function P(e,r,a){if(e==null)return"";if(typeof e=="function")return ue(e,Ve(e));if(typeof e=="string")return S(e);switch(e){case A:return S("Suspense");case U:return S("SuspenseList")}if(typeof e=="object")switch(e.$$typeof){case T:return $e(e.render);case C:return P(e.type,r,a);case I:{var t=e,s=t._payload,u=t._init;try{return P(u(s),r,a)}catch{}}}return""}var N=Object.prototype.hasOwnProperty,le={},fe=E.ReactDebugCurrentFrame;function D(e){if(e){var r=e._owner,a=P(e.type,e._source,r?r.type:null);fe.setExtraStackFrame(a)}else fe.setExtraStackFrame(null)}function Le(e,r,a,t,s){{var u=Function.call.bind(N);for(var i in e)if(u(e,i)){var n=void 0;try{if(typeof e[i]!="function"){var m=Error((t||"React class")+": "+a+" type `"+i+"` is invalid; it must be a function, usually from the `prop-types` package, but received `"+typeof e[i]+"`.This often happens because of typos such as `PropTypes.function` instead of `PropTypes.func`.");throw m.name="Invariant Violation",m}n=e[i](r,i,t,a,null,"SECRET_DO_NOT_PASS_THIS_OR_YOU_WILL_BE_FIRED")}catch(f){n=f}n&&!(n instanceof Error)&&(D(s),p("%s: type specification of %s `%s` is invalid; the type checker function must return `null` or an `Error` but returned a %s. You may have forgotten to pass an argument to the type checker creator (arrayOf, instanceOf, objectOf, oneOf, oneOfType, and shape all require an argument).",t||"React class",a,i,typeof n),D(null)),n instanceof Error&&!(n.message in le)&&(le[n.message]=!0,D(s),p("Failed %s type: %s",a,n.message),D(null))}}}var Be=Array.isArray;function Y(e){return Be(e)}function qe(e){{var r=typeof Symbol=="function"&&Symbol.toStringTag,a=r&&e[Symbol.toStringTag]||e.constructor.name||"Object";return a}}function ze(e){try{return ce(e),!1}catch{return!0}}function ce(e){return""+e}function de(e){if(ze(e))return p("The provided key is an unsupported type %s. This value must be coerced to a string before before using it here.",qe(e)),ce(e)}var x=E.ReactCurrentOwner,Ge={key:!0,ref:!0,__self:!0,__source:!0},pe,me,$;$={};function Xe(e){if(N.call(e,"ref")){var r=Object.getOwnPropertyDescriptor(e,"ref").get;if(r&&r.isReactWarning)return!1}return e.ref!==void 0}function He(e){if(N.call(e,"key")){var r=Object.getOwnPropertyDescriptor(e,"key").get;if(r&&r.isReactWarning)return!1}return e.key!==void 0}function Je(e,r){if(typeof e.ref=="string"&&x.current&&r&&x.current.stateNode!==r){var a=b(x.current.type);$[a]||(p('Component "%s" contains the string ref "%s". Support for string refs will be removed in a future major release. This case cannot be automatically converted to an arrow function. We ask you to manually fix this case by using useRef() or createRef() instead. Learn more about using refs safely here: https://reactjs.org/link/strict-mode-string-ref',b(x.current.type),e.ref),$[a]=!0)}}function Ze(e,r){{var a=function(){pe||(pe=!0,p("%s: `key` is not a prop. Trying to access it will result in `undefined` being returned. If you need to access the same value within the child component, you should pass it as a different prop. (https://reactjs.org/link/special-props)",r))};a.isReactWarning=!0,Object.defineProperty(e,"key",{get:a,configurable:!0})}}function Qe(e,r){{var a=function(){me||(me=!0,p("%s: `ref` is not a prop. Trying to access it will result in `undefined` being returned. If you need to access the same value within the child component, you should pass it as a different prop. (https://reactjs.org/link/special-props)",r))};a.isReactWarning=!0,Object.defineProperty(e,"ref",{get:a,configurable:!0})}}var er=function(e,r,a,t,s,u,i){var n={$$typeof:o,type:e,key:r,ref:a,props:i,_owner:u};return n._store={},Object.defineProperty(n._store,"validated",{configurable:!1,enumerable:!1,writable:!0,value:!1}),Object.defineProperty(n,"_self",{configurable:!1,enumerable:!1,writable:!1,value:t}),Object.defineProperty(n,"_source",{configurable:!1,enumerable:!1,writable:!1,value:s}),Object.freeze&&(Object.freeze(n.props),Object.freeze(n)),n};function rr(e,r,a,t,s){{var u,i={},n=null,m=null;a!==void 0&&(de(a),n=""+a),He(r)&&(de(r.key),n=""+r.key),Xe(r)&&(m=r.ref,Je(r,s));for(u in r)N.call(r,u)&&!Ge.hasOwnProperty(u)&&(i[u]=r[u]);if(e&&e.defaultProps){var f=e.defaultProps;for(u in f)i[u]===void 0&&(i[u]=f[u])}if(n||m){var c=typeof e=="function"?e.displayName||e.name||"Unknown":e;n&&Ze(i,c),m&&Qe(i,c)}return er(e,n,m,s,t,x.current,i)}}var V=E.ReactCurrentOwner,ve=E.ReactDebugCurrentFrame;function w(e){if(e){var r=e._owner,a=P(e.type,e._source,r?r.type:null);ve.setExtraStackFrame(a)}else ve.setExtraStackFrame(null)}var L;L=!1;function B(e){return typeof e=="object"&&e!==null&&e.$$typeof===o}function he(){{if(V.current){var e=b(V.current.type);if(e)return`

Check the render method of \``+e+"`."}return""}}function ar(e){{if(e!==void 0){var r=e.fileName.replace(/^.*[\\\/]/,""),a=e.lineNumber;return`

Check your code at `+r+":"+a+"."}return""}}var be={};function tr(e){{var r=he();if(!r){var a=typeof e=="string"?e:e.displayName||e.name;a&&(r=`

Check the top-level render call using <`+a+">.")}return r}}function _e(e,r){{if(!e._store||e._store.validated||e.key!=null)return;e._store.validated=!0;var a=tr(r);if(be[a])return;be[a]=!0;var t="";e&&e._owner&&e._owner!==V.current&&(t=" It was passed a child from "+b(e._owner.type)+"."),w(e),p('Each child in a list should have a unique "key" prop.%s%s See https://reactjs.org/link/warning-keys for more information.',a,t),w(null)}}function ge(e,r){{if(typeof e!="object")return;if(Y(e))for(var a=0;a<e.length;a++){var t=e[a];B(t)&&_e(t,r)}else if(B(e))e._store&&(e._store.validated=!0);else if(e){var s=Oe(e);if(typeof s=="function"&&s!==e.entries)for(var u=s.call(e),i;!(i=u.next()).done;)B(i.value)&&_e(i.value,r)}}}function nr(e){{var r=e.type;if(r==null||typeof r=="string")return;var a;if(typeof r=="function")a=r.propTypes;else if(typeof r=="object"&&(r.$$typeof===T||r.$$typeof===C))a=r.propTypes;else return;if(a){var t=b(r);Le(a,e.props,"prop",t,e)}else if(r.PropTypes!==void 0&&!L){L=!0;var s=b(r);p("Component %s declared `PropTypes` instead of `propTypes`. Did you misspell the property assignment?",s||"Unknown")}typeof r.getDefaultProps=="function"&&!r.getDefaultProps.isReactClassApproved&&p("getDefaultProps is only used on classic React.createClass definitions. Use a static property named `defaultProps` instead.")}}function or(e){{for(var r=Object.keys(e.props),a=0;a<r.length;a++){var t=r[a];if(t!=="children"&&t!=="key"){w(e),p("Invalid prop `%s` supplied to `React.Fragment`. React.Fragment can only have `key` and `children` props.",t),w(null);break}}e.ref!==null&&(w(e),p("Invalid attribute `ref` supplied to `React.Fragment`."),w(null))}}function ir(e,r,a,t,s,u){{var i=Ie(e);if(!i){var n="";(e===void 0||typeof e=="object"&&e!==null&&Object.keys(e).length===0)&&(n+=" You likely forgot to export your component from the file it's defined in, or you might have mixed up default and named imports.");var m=ar(s);m?n+=m:n+=he();var f;e===null?f="null":Y(e)?f="array":e!==void 0&&e.$$typeof===o?(f="<"+(b(e.type)||"Unknown")+" />",n=" Did you accidentally export a JSX literal instead of a component?"):f=typeof e,p("React.jsx: type is invalid -- expected a string (for built-in components) or a class/function (for composite components) but got: %s.%s",f,n)}var c=rr(e,r,a,s,u);if(c==null)return c;if(i){var v=r.children;if(v!==void 0)if(t)if(Y(v)){for(var R=0;R<v.length;R++)ge(v[R],e);Object.freeze&&Object.freeze(v)}else p("React.jsx: Static children should always be an array. You are likely explicitly calling React.jsxs or React.jsxDEV. Use the Babel transform instead.");else ge(v,e)}return e===y?or(c):nr(c),c}}var sr=ir;z.Fragment=y,z.jsxDEV=sr})()});var xe=q((wr,je)=>{"use strict";je.exports=Re()});var gr={};pr(gr,{default:()=>_r,frontmatter:()=>hr});var d=mr(xe()),hr={title:"Consuming and Storing Kafka Messages in Snowflake Using Kafka Connect (Part 1)",description:"How and why to use Kafka Connect to store Kafka messages in Snowflake.",date:"2024-05-16",url:"https://www.atlantbh.com/kafka-connect/",published:!0};function Te(l){let o=Object.assign({p:"p",h3:"h3",a:"a",span:"span"},l.components);return(0,d.jsxDEV)(d.Fragment,{children:[(0,d.jsxDEV)(o.p,{children:"Every year, zettabytes of data are transferred over the Internet. Managing, processing, and storing the data can be a really complex task and requires cutting-edge tools. Many tools can be used, but two technologies that are almost essential to every modern data pipeline: Kafka and Snowflake."},void 0,!1,{fileName:"/Users/darko/personal_projects/darko.sh/content/projects/_mdx_bundler_entry_point-160e8651-e27f-4fa0-b443-30dad287014a.mdx",lineNumber:9,columnNumber:1},this),`
`,(0,d.jsxDEV)(o.p,{children:"If you are planning to stream a large amount of data, Kafka is usually the best choice. Scalability, reliability, low latency and popularity are reasons why Kafka stands out. Kafka is by far the most popular event streaming platform and a go-to for many cases when it comes to streaming data."},void 0,!1,{fileName:"/Users/darko/personal_projects/darko.sh/content/projects/_mdx_bundler_entry_point-160e8651-e27f-4fa0-b443-30dad287014a.mdx",lineNumber:11,columnNumber:1},this),`
`,(0,d.jsxDEV)(o.p,{children:"Since you need to process and store the data somewhere, Snowflake is one of the top contenders for that. If you are wondering why Snowflake is so popular, look at the user satisfaction \u2014 it\u2019s over the roof. Snowflake took most of the drawbacks and oversights that other data storage solutions had and addressed it. Probably the most important feature is having the storage and compute separately scaled."},void 0,!1,{fileName:"/Users/darko/personal_projects/darko.sh/content/projects/_mdx_bundler_entry_point-160e8651-e27f-4fa0-b443-30dad287014a.mdx",lineNumber:13,columnNumber:1},this),`
`,(0,d.jsxDEV)(o.h3,{id:"what-is-kafka-connect-and-why-did-we-decide-to-use-it",children:[(0,d.jsxDEV)(o.a,{className:"subheading-anchor","aria-label":"Link to section",href:"#what-is-kafka-connect-and-why-did-we-decide-to-use-it",children:(0,d.jsxDEV)(o.span,{className:"icon icon-link"},void 0,!1,{fileName:"/Users/darko/personal_projects/darko.sh/content/projects/_mdx_bundler_entry_point-160e8651-e27f-4fa0-b443-30dad287014a.mdx"},this)},void 0,!1,{fileName:"/Users/darko/personal_projects/darko.sh/content/projects/_mdx_bundler_entry_point-160e8651-e27f-4fa0-b443-30dad287014a.mdx"},this),"What is Kafka Connect, and why did we decide to use it?"]},void 0,!0,{fileName:"/Users/darko/personal_projects/darko.sh/content/projects/_mdx_bundler_entry_point-160e8651-e27f-4fa0-b443-30dad287014a.mdx",lineNumber:15,columnNumber:1},this),`
`,(0,d.jsxDEV)(o.p,{children:"Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other data systems. It provides streaming integration between Kafka and other data sources or sinks, enabling seamless data movement and transformation. Kafka Connect uses connectors to ingest data into Kafka topics or export data from Kafka topics to external systems."},void 0,!1,{fileName:"/Users/darko/personal_projects/darko.sh/content/projects/_mdx_bundler_entry_point-160e8651-e27f-4fa0-b443-30dad287014a.mdx",lineNumber:17,columnNumber:1},this),`
`,(0,d.jsxDEV)(o.p,{children:"Kafka Connect is made for use cases like this."},void 0,!1,{fileName:"/Users/darko/personal_projects/darko.sh/content/projects/_mdx_bundler_entry_point-160e8651-e27f-4fa0-b443-30dad287014a.mdx",lineNumber:19,columnNumber:1},this),`
`,(0,d.jsxDEV)(o.p,{children:["To see how we used Kafka Connect for consuming and storing Kafka messages in Snowflake, check out the full blog (for free) at: ",(0,d.jsxDEV)(o.a,{href:"https://www.atlantbh.com/kafka-connect/",children:"https://www.atlantbh.com/kafka-connect/"},void 0,!1,{fileName:"/Users/darko/personal_projects/darko.sh/content/projects/_mdx_bundler_entry_point-160e8651-e27f-4fa0-b443-30dad287014a.mdx",lineNumber:21,columnNumber:128},this)," or ",(0,d.jsxDEV)(o.a,{href:"https://medium.com/p/f4a14a692b3a",children:"https://medium.com/p/f4a14a692b3a"},void 0,!1,{fileName:"/Users/darko/personal_projects/darko.sh/content/projects/_mdx_bundler_entry_point-160e8651-e27f-4fa0-b443-30dad287014a.mdx",lineNumber:21,columnNumber:171},this),"."]},void 0,!0,{fileName:"/Users/darko/personal_projects/darko.sh/content/projects/_mdx_bundler_entry_point-160e8651-e27f-4fa0-b443-30dad287014a.mdx",lineNumber:21,columnNumber:1},this)]},void 0,!0,{fileName:"/Users/darko/personal_projects/darko.sh/content/projects/_mdx_bundler_entry_point-160e8651-e27f-4fa0-b443-30dad287014a.mdx",lineNumber:1,columnNumber:1},this)}function br(l={}){let{wrapper:o}=l.components||{};return o?(0,d.jsxDEV)(o,Object.assign({},l,{children:(0,d.jsxDEV)(Te,l,void 0,!1,{fileName:"/Users/darko/personal_projects/darko.sh/content/projects/_mdx_bundler_entry_point-160e8651-e27f-4fa0-b443-30dad287014a.mdx"},this)}),void 0,!1,{fileName:"/Users/darko/personal_projects/darko.sh/content/projects/_mdx_bundler_entry_point-160e8651-e27f-4fa0-b443-30dad287014a.mdx"},this):Te(l)}var _r=br;return vr(gr);})();
/*! Bundled license information:

react/cjs/react-jsx-dev-runtime.development.js:
  (**
   * @license React
   * react-jsx-dev-runtime.development.js
   *
   * Copyright (c) Facebook, Inc. and its affiliates.
   *
   * This source code is licensed under the MIT license found in the
   * LICENSE file in the root directory of this source tree.
   *)
*/
;return Component;b:["$","div",null,{"className":"bg-zinc-50 min-h-screen","children":[["$","$Lc",null,{"project":{"published":true,"title":"Consuming and Storing Kafka Messages in Snowflake Using Kafka Connect (Part 1)","description":"How and why to use Kafka Connect to store Kafka messages in Snowflake.","date":"2024-05-16T00:00:00.000Z","url":"https://www.atlantbh.com/kafka-connect/","body":{"raw":"$d","code":"$e"},"_id":"projects/consuming-and-storing-kafka-messages-in-snowflake-using-kafka-connect-part1.mdx","_raw":{"sourceFilePath":"projects/consuming-and-storing-kafka-messages-in-snowflake-using-kafka-connect-part1.mdx","sourceFileName":"consuming-and-storing-kafka-messages-in-snowflake-using-kafka-connect-part1.mdx","sourceFileDir":"projects","contentType":"mdx","flattenedPath":"projects/consuming-and-storing-kafka-messages-in-snowflake-using-kafka-connect-part1"},"type":"Project","path":"/projects/consuming-and-storing-kafka-messages-in-snowflake-using-kafka-connect-part1","slug":"consuming-and-storing-kafka-messages-in-snowflake-using-kafka-connect-part1"}}],["$","article",null,{"className":"px-4 py-12 mx-auto prose prose-zinc prose-quoteless","children":["$","div",null,{"className":"mdx","children":[["$","p",null,{"className":"leading-7 [&:not(:first-child)]:mt-6","children":"Every year, zettabytes of data are transferred over the Internet. Managing, processing, and storing the data can be a really complex task and requires cutting-edge tools. Many tools can be used, but two technologies that are almost essential to every modern data pipeline: Kafka and Snowflake."}],"\n",["$","p",null,{"className":"leading-7 [&:not(:first-child)]:mt-6","children":"If you are planning to stream a large amount of data, Kafka is usually the best choice. Scalability, reliability, low latency and popularity are reasons why Kafka stands out. Kafka is by far the most popular event streaming platform and a go-to for many cases when it comes to streaming data."}],"\n",["$","p",null,{"className":"leading-7 [&:not(:first-child)]:mt-6","children":"Since you need to process and store the data somewhere, Snowflake is one of the top contenders for that. If you are wondering why Snowflake is so popular, look at the user satisfaction — it’s over the roof. Snowflake took most of the drawbacks and oversights that other data storage solutions had and addressed it. Probably the most important feature is having the storage and compute separately scaled."}],"\n",["$","h3",null,{"className":"mt-8 scroll-m-20 text-2xl font-semibold tracking-tight","id":"what-is-kafka-connect-and-why-did-we-decide-to-use-it","children":[["$","$Lf",null,{"className":"font-medium text-zinc-900 underline underline-offset-4 subheading-anchor","aria-label":"Link to section","href":"#what-is-kafka-connect-and-why-did-we-decide-to-use-it","children":["$","span",null,{"className":"icon icon-link"}]}],"What is Kafka Connect, and why did we decide to use it?"]}],"\n",["$","p",null,{"className":"leading-7 [&:not(:first-child)]:mt-6","children":"Kafka Connect is a tool for scalably and reliably streaming data between Apache Kafka and other data systems. It provides streaming integration between Kafka and other data sources or sinks, enabling seamless data movement and transformation. Kafka Connect uses connectors to ingest data into Kafka topics or export data from Kafka topics to external systems."}],"\n",["$","p",null,{"className":"leading-7 [&:not(:first-child)]:mt-6","children":"Kafka Connect is made for use cases like this."}],"\n",["$","p",null,{"className":"leading-7 [&:not(:first-child)]:mt-6","children":["To see how we used Kafka Connect for consuming and storing Kafka messages in Snowflake, check out the full blog (for free) at: ",["$","$Lf",null,{"className":"font-medium text-zinc-900 underline underline-offset-4","href":"https://www.atlantbh.com/kafka-connect/","children":"https://www.atlantbh.com/kafka-connect/"}]," or ",["$","$Lf",null,{"className":"font-medium text-zinc-900 underline underline-offset-4","href":"https://medium.com/p/f4a14a692b3a","children":"https://medium.com/p/f4a14a692b3a"}],"."]}]]}]}]]}]
a:null
